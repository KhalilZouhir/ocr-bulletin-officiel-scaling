{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30485cba-f63f-49fe-8761-f2753d2e1367",
   "metadata": {},
   "source": [
    "# Full guide on how to run the OCR of \"bulletin officiel\" locally "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de20ea66-8a67-466b-b860-debb41560b5c",
   "metadata": {},
   "source": [
    "### 1- create the khalil_vllm VENV check README"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6218df-a850-4072-9467-4b30bf30eac7",
   "metadata": {},
   "source": [
    "### 2- run in a cmd \n",
    "vllm serve /CHANDRA/MODEL/PATH \\\n",
    " --max-model-len 122000 \\\n",
    "  --tensor-parallel-size 1 \\\n",
    "  --port 9996\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94107b8c-2336-4f56-aeb9-bd96c49d569b",
   "metadata": {},
   "source": [
    "### 3- execute the following python code to split the documents + run ocr using chandra please adapt paths \n",
    "#### but first create \"processing_results\" + \"pdf_documents\" folders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edec0b1c-3b45-495d-9992-0df6b97f1a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# UNIFIED PDF OCR PIPELINE WITH COLUMN SPLITTING\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "import base64\n",
    "import requests\n",
    "import glob\n",
    "import math\n",
    "import shutil\n",
    "import cv2\n",
    "import numpy as np\n",
    "from typing import Tuple, Union, Dict, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import fitz  # PyMuPDF\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "# VLLM API Configuration\n",
    "VLLM_API_URL = \"http://localhost:9996/v1/chat/completions\"\n",
    "MODEL_PATH = \"/home/skiredj.abderrahman/khalil/chandra\"\n",
    "\n",
    "# LLM Generation Parameters\n",
    "GENERATION_PARAMS = {\n",
    "    \"temperature\": 0.1,\n",
    "    \"max_tokens\": 30000,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "\n",
    "# Request timeout\n",
    "REQUEST_TIMEOUT_SECONDS = 180\n",
    "\n",
    "# Parallel processing\n",
    "NUM_WORKERS = 17\n",
    "\n",
    "# OCR Prompt\n",
    "OCR_PROMPT = \"\"\"Extract all text exactly as it appears in this image.\n",
    "Preserve the original layout, reading order, formatting, and structure.\n",
    "Keep all tables as proper Markdown tables.\n",
    "Maintain all mathematical formulas, equations, and special characters.\n",
    "Do not rewrite, summarize, or modify any content.\n",
    "If any text is unclear or unreadable, write [UNCLEAR].\n",
    "Output only the extracted text without any preamble or explanation.\"\"\"\n",
    "\n",
    "# Folders\n",
    "PDF_FOLDER = r\"/home/skiredj.abderrahman/khalil/OCR_scaling_bulletin_officiel/batch/pdf_documents\"\n",
    "PROCESSING_BASE = r\"/home/skiredj.abderrahman/khalil/OCR_scaling_bulletin_officiel/batch/processing_results\"\n",
    "OUTPUT_FOLDER = os.path.join(PROCESSING_BASE, \"documents_transformed_to_markdown\")\n",
    "TEMP_FOLDER = os.path.join(PROCESSING_BASE, \"tempo\")\n",
    "\n",
    "# ========== IMAGE PREPROCESSING FUNCTIONS (FROM CODE 2) ==========\n",
    "\n",
    "def normalize_illumination(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (0, 0), sigmaX=25, sigmaY=25)\n",
    "    norm = cv2.divide(gray, blur, scale=255)\n",
    "    return cv2.cvtColor(norm, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "def enhance_contrast_and_sharpness(img):\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "    l2 = clahe.apply(l)\n",
    "    lab = cv2.merge((l2, a, b))\n",
    "    enhanced = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "    kernel = np.array([[0, -1, 0],\n",
    "                       [-1, 5, -1],\n",
    "                       [0, -1, 0]])\n",
    "    sharp = cv2.filter2D(enhanced, -1, kernel)\n",
    "    return sharp\n",
    "\n",
    "def upscale_opencv(img, scale=2):\n",
    "    h, w = img.shape[:2]\n",
    "    upscaled = cv2.resize(img, (w * scale, h * scale),\n",
    "                          interpolation=cv2.INTER_CUBIC)\n",
    "    return upscaled\n",
    "\n",
    "def preprocess_page(img):\n",
    "    img = normalize_illumination(img)\n",
    "    img = enhance_contrast_and_sharpness(img)\n",
    "    img = upscale_opencv(img, scale=2)\n",
    "    return img\n",
    "\n",
    "def _read_image(image_or_path: Union[str, np.ndarray]) -> np.ndarray:\n",
    "    if isinstance(image_or_path, str):\n",
    "        img = cv2.imread(image_or_path, cv2.IMREAD_COLOR)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Impossible de lire l'image : {image_or_path}\")\n",
    "    elif isinstance(image_or_path, np.ndarray):\n",
    "        img = image_or_path.copy()\n",
    "    else:\n",
    "        raise TypeError(\"image_or_path doit √™tre un chemin ou une image numpy.\")\n",
    "    img = preprocess_page(img)\n",
    "    return img\n",
    "\n",
    "# ========== TABLE DETECTION (FROM CODE 2) ==========\n",
    "\n",
    "def is_big_table(img, debug=False, base_name=\"\"):\n",
    "    H, W = img.shape[:2]\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _, th = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    bin_img = th if np.mean(th) > 127 else cv2.bitwise_not(th)\n",
    "    bin_img = cv2.medianBlur(bin_img, 3)\n",
    "\n",
    "    # Vertical lines\n",
    "    vert_len = max(10, H // 20)\n",
    "    vert_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, vert_len))\n",
    "    vertical = cv2.morphologyEx(255 - bin_img, cv2.MORPH_OPEN, vert_kernel, iterations=1)\n",
    "    vertical = cv2.morphologyEx(vertical, cv2.MORPH_CLOSE, vert_kernel, iterations=1)\n",
    "    _, vertical = cv2.threshold(vertical, 0, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    table_big_min_height_ratio = 0.85\n",
    "    table_many_verticals = 5\n",
    "    table_min_spread_ratio = 0.5\n",
    "    table_min_width_coverage = 0.06\n",
    "\n",
    "    tall_verticals = []\n",
    "    total_v_width = 0\n",
    "    contours_vert, _ = cv2.findContours(vertical, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    for c in contours_vert:\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "        if h >= int(table_big_min_height_ratio * H) and (1 <= w <= int(0.03 * W)):\n",
    "            tall_verticals.append((x, y, w, h))\n",
    "            total_v_width += w\n",
    "\n",
    "    spread_ok = False\n",
    "    if len(tall_verticals) >= table_many_verticals:\n",
    "        xs = np.array([x + w / 2.0 for x, _, w, _ in tall_verticals], dtype=float)\n",
    "        width_spread = np.ptp(xs) if xs.size else 0.0\n",
    "        if width_spread >= table_min_spread_ratio * W:\n",
    "            spread_ok = True\n",
    "\n",
    "    width_coverage = (total_v_width / float(W)) if W > 0 else 0.0\n",
    "    looks_like_big_table = (\n",
    "            (len(tall_verticals) >= table_many_verticals)\n",
    "            and spread_ok\n",
    "            and (width_coverage >= table_min_width_coverage)\n",
    "    )\n",
    "\n",
    "    # Horizontal lines\n",
    "    horiz_len = max(10, W // 20)\n",
    "    horiz_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (horiz_len, 1))\n",
    "    horizontal = cv2.morphologyEx(255 - bin_img, cv2.MORPH_OPEN, horiz_kernel, iterations=1)\n",
    "    horizontal = cv2.morphologyEx(horizontal, cv2.MORPH_CLOSE, horiz_kernel, iterations=1)\n",
    "    _, horizontal = cv2.threshold(horizontal, 0, 255, cv2.THRESH_BINARY)\n",
    "    contours_horiz, _ = cv2.findContours(horizontal, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    long_horizontal_lines = []\n",
    "    medium_horizontal_lines = []\n",
    "\n",
    "    for c in contours_horiz:\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "        line_width_ratio = w / W\n",
    "        if line_width_ratio > 0.5:\n",
    "            long_horizontal_lines.append((x, y, w, h))\n",
    "        elif line_width_ratio > 0.2:\n",
    "            medium_horizontal_lines.append((x, y, w, h))\n",
    "\n",
    "    many_horizontal = len(long_horizontal_lines)\n",
    "    medium_horizontal = len(medium_horizontal_lines)\n",
    "\n",
    "    line_density = many_horizontal / (H / 100.0)\n",
    "\n",
    "    if long_horizontal_lines:\n",
    "        line_ys = [y + h / 2 for x, y, w, h in long_horizontal_lines]\n",
    "        vertical_spread = (max(line_ys) - min(line_ys)) / H if len(line_ys) > 1 else 0\n",
    "    else:\n",
    "        vertical_spread = 0\n",
    "\n",
    "    spacing_regularity = 0\n",
    "    if len(long_horizontal_lines) >= 3:\n",
    "        line_ys = sorted([y + h / 2 for x, y, w, h in long_horizontal_lines])\n",
    "        spacings = [line_ys[i + 1] - line_ys[i] for i in range(len(line_ys) - 1)]\n",
    "        if spacings:\n",
    "            avg_spacing = np.mean(spacings)\n",
    "            spacing_variance = np.var(spacings) / (avg_spacing + 1)\n",
    "            spacing_regularity = 1.0 / (1.0 + spacing_variance)\n",
    "\n",
    "    text_pixels = np.sum(255 - bin_img > 127)\n",
    "    line_pixels = np.sum(horizontal > 127) + np.sum(vertical > 127)\n",
    "    line_to_text_ratio = line_pixels / (text_pixels + 1)\n",
    "\n",
    "    old_table_detection = many_horizontal >= 4\n",
    "\n",
    "    dense_horizontal_table = (\n",
    "            many_horizontal >= 5 and\n",
    "            line_density >= 2.0 and\n",
    "            vertical_spread >= 0.6\n",
    "    )\n",
    "\n",
    "    regular_table = (\n",
    "            many_horizontal >= 4 and\n",
    "            spacing_regularity >= 0.7 and\n",
    "            vertical_spread >= 0.5\n",
    "    )\n",
    "\n",
    "    line_heavy_document = (\n",
    "            many_horizontal >= 5 and\n",
    "            line_to_text_ratio >= 0.25 and\n",
    "            (medium_horizontal + many_horizontal) >= 12 and\n",
    "            vertical_spread >= 0.4\n",
    "    )\n",
    "\n",
    "    table_detected = (\n",
    "            looks_like_big_table or\n",
    "            old_table_detection or\n",
    "            dense_horizontal_table or\n",
    "            regular_table or\n",
    "            line_heavy_document\n",
    "    )\n",
    "\n",
    "    return table_detected\n",
    "\n",
    "# ========== COLUMN SPLITTING FUNCTIONS (FROM CODE 2) ==========\n",
    "\n",
    "def has_significant_text(img_region, min_text_ratio=0.01):\n",
    "    if len(img_region.shape) == 3:\n",
    "        gray = cv2.cvtColor(img_region, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = img_region\n",
    "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    text_pixels = np.sum(binary > 0)\n",
    "    total_pixels = binary.shape[0] * binary.shape[1]\n",
    "    text_ratio = text_pixels / total_pixels\n",
    "    return text_ratio > min_text_ratio\n",
    "\n",
    "def find_text_density_split(img, debug=False):\n",
    "    if len(img.shape) == 3:\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = img\n",
    "\n",
    "    H, W = gray.shape\n",
    "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (max(40, W // 15), 1))\n",
    "    horizontal_lines = cv2.morphologyEx(binary, cv2.MORPH_OPEN, horizontal_kernel)\n",
    "    h_contours, _ = cv2.findContours(horizontal_lines, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    long_h_lines = sum(1 for c in h_contours if cv2.boundingRect(c)[2] > 0.7 * W)\n",
    "    medium_h_lines = sum(1 for c in h_contours if 0.4 * W < cv2.boundingRect(c)[2] <= 0.7 * W)\n",
    "    total_h_lines = long_h_lines + medium_h_lines\n",
    "\n",
    "    vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, max(30, H // 20)))\n",
    "    vertical_lines = cv2.morphologyEx(binary, cv2.MORPH_OPEN, vertical_kernel)\n",
    "    v_contours, _ = cv2.findContours(vertical_lines, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    long_v_lines = sum(1 for c in v_contours if cv2.boundingRect(c)[3] > 0.6 * H)\n",
    "    medium_v_lines = sum(1 for c in v_contours if 0.3 * H < cv2.boundingRect(c)[3] <= 0.6 * H)\n",
    "    total_v_lines = long_v_lines + medium_v_lines\n",
    "\n",
    "    if total_h_lines >= 2 and total_v_lines >= 2:\n",
    "        grid_mask = cv2.bitwise_and(horizontal_lines, vertical_lines)\n",
    "        intersection_contours, _ = cv2.findContours(grid_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        intersections = len(intersection_contours)\n",
    "        intersections_per_area = intersections / ((W * H) / 10000)\n",
    "\n",
    "        if intersections > 0:\n",
    "            intersection_points = []\n",
    "            for c in intersection_contours:\n",
    "                x, y, w, h = cv2.boundingRect(c)\n",
    "                intersection_points.append((x + w // 2, y + h // 2))\n",
    "\n",
    "            if len(intersection_points) >= 4:\n",
    "                distances = []\n",
    "                for i, (x1, y1) in enumerate(intersection_points):\n",
    "                    for j, (x2, y2) in enumerate(intersection_points[i + 1:], i + 1):\n",
    "                        dist = ((x2 - x1) ** 2 + (y2 - y1) ** 2) ** 0.5\n",
    "                        distances.append(dist)\n",
    "\n",
    "                if distances:\n",
    "                    dist_variance = np.var(distances)\n",
    "                    dist_mean = np.mean(distances)\n",
    "                    regularity_score = dist_variance / (dist_mean ** 2) if dist_mean > 0 else 0\n",
    "                else:\n",
    "                    regularity_score = 0\n",
    "            else:\n",
    "                regularity_score = 0\n",
    "        else:\n",
    "            regularity_score = 0\n",
    "            intersections_per_area = 0\n",
    "\n",
    "        if intersections >= 20 and regularity_score < 0.1 and intersections_per_area > 0.3:\n",
    "            return None\n",
    "        if total_h_lines >= 6 and intersections >= 15 and intersections_per_area > 0.2:\n",
    "            return None\n",
    "        if total_h_lines >= 4 and total_v_lines >= 4 and intersections >= 25:\n",
    "            return None\n",
    "\n",
    "    vertical_projection = np.sum(binary, axis=0) / 255\n",
    "    window_size = max(1, int(W * 0.02))\n",
    "    cand_points = []\n",
    "    maxpv = vertical_projection.max() if vertical_projection.size else 0.0\n",
    "\n",
    "    for x in range(window_size, W - window_size):\n",
    "        a = max(0, x - window_size // 2)\n",
    "        b = min(W, x + window_size // 2)\n",
    "        window_density = np.mean(vertical_projection[a:b])\n",
    "        if window_density < maxpv * 0.1:\n",
    "            cand_points.append((x, window_density))\n",
    "\n",
    "    if not cand_points:\n",
    "        return None\n",
    "\n",
    "    min_center = int(0.25 * W)\n",
    "    max_center = int(0.75 * W)\n",
    "    cand_points = [p for p in cand_points if min_center <= p[0] <= max_center]\n",
    "    if not cand_points:\n",
    "        return None\n",
    "\n",
    "    center_x = W // 2\n",
    "    best_split = min(cand_points, key=lambda p: abs(p[0] - center_x))[0]\n",
    "\n",
    "    def text_ratio(region):\n",
    "        if len(region.shape) == 3:\n",
    "            rg = cv2.cvtColor(region, cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            rg = region\n",
    "        _, b = cv2.threshold(rg, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "        return (np.sum(b > 0) / float(b.size)) if b.size else 0.0\n",
    "\n",
    "    left_ratio = text_ratio(img[:, :best_split])\n",
    "    right_ratio = text_ratio(img[:, best_split:])\n",
    "\n",
    "    min_text_ratio = 0.01\n",
    "    balance_min_factor = 0.5\n",
    "\n",
    "    if left_ratio < min_text_ratio or right_ratio < min_text_ratio:\n",
    "        return None\n",
    "\n",
    "    if min(left_ratio, right_ratio) < balance_min_factor * max(left_ratio, right_ratio):\n",
    "        return None\n",
    "\n",
    "    return best_split\n",
    "\n",
    "def has_unique_central_vertical_rule(\n",
    "    image_or_path: Union[str, np.ndarray],\n",
    "    center_band_ratio: float = 0.20,\n",
    "    min_height_ratio: float = 0.70,\n",
    "    max_line_width_ratio: float = 0.03,\n",
    "    min_line_width_px: int = 1,\n",
    "    max_gap_ratio: float = 0.10,\n",
    "    max_gap_runs: int = 2,\n",
    "    gap_run_ratio: float = 0.01,\n",
    "    max_candidates_allowed: int = 3,\n",
    "    debug: bool = False,\n",
    "    base_name: str = \"\"\n",
    ") -> Tuple[bool, Dict]:\n",
    "    bgr_orig = _read_image(image_or_path)\n",
    "    orig_H, orig_W = bgr_orig.shape[:2]\n",
    "    scale_factor = 1.0\n",
    "    max_side = max(orig_H, orig_W)\n",
    "    bgr = bgr_orig\n",
    "    if max_side > 2000:\n",
    "        scale_factor = 2000.0 / max_side\n",
    "        bgr = cv2.resize(\n",
    "            bgr_orig, (int(orig_W*scale_factor), int(orig_H*scale_factor)),\n",
    "            interpolation=cv2.INTER_AREA\n",
    "        )\n",
    "    H, W = bgr.shape[:2]\n",
    "    gray = cv2.cvtColor(bgr, cv2.COLOR_BGR2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    gray = clahe.apply(gray)\n",
    "    blur = cv2.GaussianBlur(gray, (3,3), 0)\n",
    "    _, th_otsu = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    bin_img = th_otsu if np.mean(th_otsu) > 127 else cv2.bitwise_not(th_otsu)\n",
    "    bin_img = cv2.medianBlur(bin_img, 3)\n",
    "    vert_len = max(10, H // 20)\n",
    "    vert_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, vert_len))\n",
    "    vertical = cv2.morphologyEx(255 - bin_img, cv2.MORPH_OPEN, vert_kernel, iterations=1)\n",
    "    vertical = cv2.morphologyEx(vertical, cv2.MORPH_CLOSE, vert_kernel, iterations=1)\n",
    "    _, vertical = cv2.threshold(vertical, 0, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    table_big_min_height_ratio = 0.85\n",
    "    table_many_verticals = 5\n",
    "    table_min_spread_ratio = 0.50\n",
    "    table_min_width_coverage = 0.06\n",
    "    tall_verticals = []\n",
    "    total_v_width = 0\n",
    "    contours_all, _ = cv2.findContours(vertical, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for c in contours_all:\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "        if h >= int(table_big_min_height_ratio * H) and (min_line_width_px <= w <= int(max_line_width_ratio * W)):\n",
    "            tall_verticals.append((x, y, w, h))\n",
    "            total_v_width += w\n",
    "    spread_ok = False\n",
    "    if len(tall_verticals) >= table_many_verticals:\n",
    "        xs = np.array([x + w/2.0 for x,_,w,_ in tall_verticals], dtype=float)\n",
    "        width_spread = np.ptp(xs) if xs.size else 0.0\n",
    "        if width_spread >= table_min_spread_ratio * W:\n",
    "            spread_ok = True\n",
    "    width_coverage = (total_v_width / float(W)) if W > 0 else 0.0\n",
    "    looks_like_big_table = (len(tall_verticals) >= table_many_verticals) and spread_ok and (width_coverage >= table_min_width_coverage)\n",
    "\n",
    "    mid = W // 2\n",
    "    band_half = int(center_band_ratio * W)\n",
    "    x0 = max(0, mid - band_half)\n",
    "    x1 = min(W, mid + band_half)\n",
    "    central_band = np.zeros_like(vertical)\n",
    "    central_band[:, x0:x1] = vertical[:, x0:x1]\n",
    "    contours, _ = cv2.findContours(central_band, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    candidates = []\n",
    "    \n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        if h < int(min_height_ratio * H):\n",
    "            continue\n",
    "        if w < min_line_width_px or w > int(max_line_width_ratio * W):\n",
    "            continue\n",
    "        x_center = x + w / 2.0\n",
    "        if not (mid - band_half <= x_center <= mid + band_half):\n",
    "            continue\n",
    "        roi = (255 - bin_img)[y:y+h, max(0, x):min(W, x+w)]\n",
    "        row_has_ink = (np.max(roi, axis=1) > 0).astype(np.uint8)\n",
    "        min_gap = max(3, int(gap_run_ratio * H))\n",
    "        gaps, run = [], 0\n",
    "        for val in row_has_ink:\n",
    "            if val == 0:\n",
    "                run += 1\n",
    "            else:\n",
    "                if run >= min_gap:\n",
    "                    gaps.append(run)\n",
    "                run = 0\n",
    "        if run >= min_gap:\n",
    "            gaps.append(run)\n",
    "        total_gap = sum(gaps)\n",
    "        if total_gap > max_gap_ratio * H or len(gaps) > max_gap_runs:\n",
    "            continue\n",
    "        candidates.append((x, y, w, h, {\"gaps\": gaps, \"total_gap\": total_gap}))\n",
    "\n",
    "    central_tall_verticals = 0\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        if h >= int(min_height_ratio * H) and (min_line_width_px <= w <= int(max_line_width_ratio * W)):\n",
    "            central_tall_verticals += 1\n",
    "    if central_tall_verticals > 1:\n",
    "        return (False, {\n",
    "            \"original_image_size\": (orig_H, orig_W),\n",
    "            \"processed_image_size\": (H, W),\n",
    "            \"scale_factor\": scale_factor,\n",
    "            \"center_band\": (x0, x1),\n",
    "            \"candidates_found\": len(candidates),\n",
    "            \"central_tall_verticals\": central_tall_verticals,\n",
    "            \"table_blocked\": True,\n",
    "        })\n",
    "\n",
    "    global_like = 0\n",
    "    for c in contours_all:\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "        if h >= int(min_height_ratio * H) and (min_line_width_px <= w <= int(max_line_width_ratio * W)):\n",
    "            global_like += 1\n",
    "\n",
    "    details = {\n",
    "        \"original_image_size\": (orig_H, orig_W),\n",
    "        \"processed_image_size\": (H, W),\n",
    "        \"scale_factor\": scale_factor,\n",
    "        \"center_band\": (x0, x1),\n",
    "        \"candidates_found\": len(candidates),\n",
    "        \"global_vertical_candidates\": global_like,\n",
    "        \"candidates\": [{\"bbox\": (int(x), int(y), int(w), int(h)), **stats}\n",
    "                       for (x, y, w, h, stats) in candidates],\n",
    "        \"big_table_heuristic\": {\n",
    "            \"looks_like_big_table\": bool(looks_like_big_table),\n",
    "            \"very_tall_verticals\": int(len(tall_verticals)),\n",
    "            \"spread_ok\": bool(spread_ok),\n",
    "            \"width_coverage\": float(width_coverage)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if global_like > max_candidates_allowed:\n",
    "        return (False, details)\n",
    "    if looks_like_big_table:\n",
    "        return (False, details)\n",
    "    is_true = (len(candidates) == 1)\n",
    "    return (is_true, details)\n",
    "\n",
    "def get_split_x_from_details(details: Dict) -> Optional[int]:\n",
    "    if details.get(\"candidates_found\", 0) != 1:\n",
    "        return None\n",
    "    (x, y, w, h) = details[\"candidates\"][0][\"bbox\"]\n",
    "    split_x_processed = int(round(x + w / 2))\n",
    "    scale = float(details.get(\"scale_factor\", 1.0))\n",
    "    if scale <= 0:\n",
    "        scale = 1.0\n",
    "    split_x_original = int(round(split_x_processed / scale))\n",
    "    orig_W = details.get(\"original_image_size\", (0, 0))[1]\n",
    "    if orig_W:\n",
    "        split_x_original = max(1, min(orig_W - 1, split_x_original))\n",
    "    return split_x_original\n",
    "\n",
    "def _derive_names(image_or_path: Union[str, np.ndarray],\n",
    "                  base_name: Optional[str]) -> Tuple[str, str]:\n",
    "    if isinstance(image_or_path, str):\n",
    "        stem, ext = os.path.splitext(os.path.basename(image_or_path))\n",
    "        if ext == \"\":\n",
    "            ext = \".png\"\n",
    "        return stem, ext\n",
    "    else:\n",
    "        if not base_name:\n",
    "            raise ValueError(\"For ndarray input, please provide base_name.\")\n",
    "        stem, ext = os.path.splitext(base_name)\n",
    "        if ext == \"\":\n",
    "            ext = \".png\"\n",
    "        return stem, ext\n",
    "\n",
    "def perform_split(img, split_x, dest_dir, stem, ext, left_suffix, right_suffix, details):\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    left_img = img[:, :split_x]\n",
    "    right_img = img[:, split_x:]\n",
    "    left_path = os.path.join(dest_dir, f\"{stem}{left_suffix}{ext}\")\n",
    "    right_path = os.path.join(dest_dir, f\"{stem}{right_suffix}{ext}\")\n",
    "    ok_left = cv2.imwrite(left_path, left_img)\n",
    "    ok_right = cv2.imwrite(right_path, right_img)\n",
    "    if ok_left and ok_right:\n",
    "        return {\n",
    "            \"split\": True,\n",
    "            \"left_path\": left_path,\n",
    "            \"right_path\": right_path,\n",
    "            \"split_x\": split_x,\n",
    "            \"details\": details,\n",
    "        }\n",
    "    else:\n",
    "        out_path = os.path.join(dest_dir, f\"{stem}{ext}\")\n",
    "        cv2.imwrite(out_path, img)\n",
    "        return {\n",
    "            \"split\": False,\n",
    "            \"reason\": \"write_error\",\n",
    "            \"copied_path\": out_path\n",
    "        }\n",
    "\n",
    "def enhanced_split_for_administrative_docs(\n",
    "        image_or_path: Union[str, np.ndarray],\n",
    "        dest_dir: str,\n",
    "        base_name: Optional[str] = None,\n",
    "        left_suffix: str = \"_left\",\n",
    "        right_suffix: str = \"_right\",\n",
    "        debug=False\n",
    ") -> Dict:\n",
    "    stem, ext = _derive_names(image_or_path, base_name)\n",
    "    img = _read_image(image_or_path)\n",
    "    orig_H, orig_W = img.shape[:2]\n",
    "\n",
    "    is_two_col, details = has_unique_central_vertical_rule(\n",
    "        image_or_path, debug=debug, base_name=stem)\n",
    "\n",
    "    if is_two_col:\n",
    "        split_x = get_split_x_from_details(details)\n",
    "        if split_x is not None:\n",
    "            split_x = max(1, min(orig_W - 1, int(split_x)))\n",
    "            left_has_text = has_significant_text(img[:, :split_x])\n",
    "            right_has_text = has_significant_text(img[:, split_x:])\n",
    "\n",
    "            if left_has_text and right_has_text:\n",
    "                return perform_split(img, split_x, dest_dir, stem, ext, left_suffix, right_suffix, details)\n",
    "\n",
    "    alternative_split_x = find_text_density_split(img, debug=debug)\n",
    "    if alternative_split_x is not None:\n",
    "        left_has_text = has_significant_text(img[:, :alternative_split_x])\n",
    "        right_has_text = has_significant_text(img[:, alternative_split_x:])\n",
    "\n",
    "        if left_has_text and right_has_text:\n",
    "            return perform_split(img, alternative_split_x, dest_dir, stem, ext, left_suffix, right_suffix,\n",
    "                                 {\"method\": \"text_density\", \"split_x\": alternative_split_x})\n",
    "\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    out_path = os.path.join(dest_dir, f\"{stem}{ext}\")\n",
    "    if isinstance(image_or_path, str) and os.path.isfile(image_or_path):\n",
    "        shutil.copy2(image_or_path, out_path)\n",
    "    else:\n",
    "        cv2.imwrite(out_path, img)\n",
    "\n",
    "    return {\n",
    "        \"split\": False,\n",
    "        \"reason\": \"no_valid_split_both_sides\",\n",
    "        \"copied_path\": out_path\n",
    "    }\n",
    "\n",
    "def split_if_two_column(\n",
    "        image_or_path,\n",
    "        dest_dir,\n",
    "        base_name=None,\n",
    "        debug=False\n",
    "):\n",
    "    img = _read_image(image_or_path)\n",
    "    stem, ext = _derive_names(image_or_path, base_name)\n",
    "\n",
    "    if is_big_table(img, debug=debug, base_name=stem):\n",
    "        if debug:\n",
    "            print(\"üõë Table d√©tect√©e : aucun split.\")\n",
    "        out_path = os.path.join(dest_dir, f\"{stem}{ext}\")\n",
    "        if isinstance(image_or_path, str) and os.path.isfile(image_or_path):\n",
    "            shutil.copy2(image_or_path, out_path)\n",
    "        else:\n",
    "            cv2.imwrite(out_path, img)\n",
    "        return {\n",
    "            \"split\": False,\n",
    "            \"reason\": \"big_table_detected\",\n",
    "            \"copied_path\": out_path\n",
    "        }\n",
    "\n",
    "    return enhanced_split_for_administrative_docs(\n",
    "        image_or_path, dest_dir, base_name=base_name, debug=debug\n",
    "    )\n",
    "\n",
    "# ========== OCR FUNCTIONS (FROM CODE 1) ==========\n",
    "\n",
    "def call_vllm_ocr(image_path, retries=3):\n",
    "    attempt = 0\n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            with open(image_path, \"rb\") as f:\n",
    "                img_base64 = base64.b64encode(f.read()).decode('utf-8')\n",
    "            \n",
    "            payload = {\n",
    "                \"model\": MODEL_PATH,\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"image_url\",\n",
    "                                \"image_url\": {\n",
    "                                    \"url\": f\"data:image/png;base64,{img_base64}\"\n",
    "                                }\n",
    "                            },\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": OCR_PROMPT\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                **GENERATION_PARAMS\n",
    "            }\n",
    "            \n",
    "            response = requests.post(\n",
    "                VLLM_API_URL,\n",
    "                json=payload,\n",
    "                timeout=REQUEST_TIMEOUT_SECONDS\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            result = response.json()\n",
    "            \n",
    "            if 'choices' in result and result['choices']:\n",
    "                extracted_text = result['choices'][0]['message']['content']\n",
    "                return extracted_text\n",
    "            else:\n",
    "                raise Exception(f\"Unexpected response format: {result}\")\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            attempt += 1\n",
    "            print(f\"Timeout on attempt {attempt}/{retries} for {image_path}\")\n",
    "            if attempt < retries:\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                raise Exception(f\"Request timed out after {retries} attempts\")\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            attempt += 1\n",
    "            print(f\"Request error on attempt {attempt}/{retries} for {image_path}: {e}\")\n",
    "            if attempt < retries:\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                raise Exception(f\"Request failed after {retries} attempts: {e}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            print(f\"Error on attempt {attempt}/{retries} for {image_path}: {e}\")\n",
    "            if attempt < retries:\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "def process_image(input_image_path, output_text_path, retries=3):\n",
    "    if os.path.exists(output_text_path):\n",
    "        print(f\"Skipping already processed file: {output_text_path}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        print(f\"Processing image: {input_image_path}\")\n",
    "        extracted_text = call_vllm_ocr(input_image_path, retries=retries)\n",
    "        with open(output_text_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(extracted_text)\n",
    "        print(f\"Finished processing: {input_image_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {input_image_path}: {e}\")\n",
    "        with open(output_text_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"[OCR FAILED: {str(e)}]\")\n",
    "\n",
    "def clean_unnecessary_linebreaks(text):\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_text = []\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        if lines[i].startswith(\"=== Page\"):\n",
    "            cleaned_text.append(lines[i])\n",
    "            if i + 1 < len(lines) and lines[i + 1].strip() == \"\":\n",
    "                cleaned_text.append(\"\")\n",
    "        else:\n",
    "            if (i > 0 and\n",
    "                not lines[i - 1].startswith(\"=== Page\") and\n",
    "                not lines[i - 1].rstrip().endswith('.')):\n",
    "                if cleaned_text and cleaned_text[-1] != \"\":\n",
    "                    cleaned_text[-1] = cleaned_text[-1].rstrip() + \" \" + lines[i].lstrip()\n",
    "                else:\n",
    "                    cleaned_text.append(lines[i])\n",
    "            else:\n",
    "                cleaned_text.append(lines[i])\n",
    "\n",
    "    return '\\n'.join(cleaned_text)\n",
    "\n",
    "# ========== MAIN UNIFIED PIPELINE ==========\n",
    "\n",
    "def pdf_to_images_with_split(pdf_path, temp_img_dir, split_dir, pdf_base_name, zoom=4, debug=False):\n",
    "    \"\"\"\n",
    "    Convert PDF to images and apply column splitting\n",
    "    \"\"\"\n",
    "    os.makedirs(temp_img_dir, exist_ok=True)\n",
    "    os.makedirs(split_dir, exist_ok=True)\n",
    "    \n",
    "    # Step 1: Convert PDF to images\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    for page_number in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_number)\n",
    "        mat = fitz.Matrix(zoom, zoom)\n",
    "        pix = page.get_pixmap(matrix=mat)\n",
    "        image_filename = f\"{pdf_base_name}_{page_number + 1}.png\"\n",
    "        image_path = os.path.join(temp_img_dir, image_filename)\n",
    "        pix.save(image_path)\n",
    "    pdf_document.close()\n",
    "    \n",
    "    # Step 2: Apply column splitting to each image\n",
    "    image_files = sorted(glob.glob(os.path.join(temp_img_dir, \"*.png\")))\n",
    "    for img_path in image_files:\n",
    "        result = split_if_two_column(img_path, split_dir, debug=debug)\n",
    "        if debug:\n",
    "            base_name = os.path.basename(img_path)\n",
    "            print(f\"[{base_name}] Split result: {result.get('split')}\")\n",
    "\n",
    "def ocr_split_images(split_dir, pdf_base_name, output_md_path):\n",
    "    \"\"\"\n",
    "    Run OCR on split images and generate markdown output\n",
    "    \"\"\"\n",
    "    txt_dir = os.path.join(TEMP_FOLDER, f\"tempo_res_{pdf_base_name}\")\n",
    "    os.makedirs(txt_dir, exist_ok=True)\n",
    "    \n",
    "    # Get all images from split directory\n",
    "    # Get all images from split directory\n",
    "    image_files = glob.glob(os.path.join(split_dir, \"*.png\"))\n",
    "    \n",
    "    # Custom sort: for Arabic, process RIGHT pages before LEFT pages\n",
    "    def arabic_sort_key(path):\n",
    "        filename = os.path.basename(path)\n",
    "        \n",
    "        # Extract page number using regex\n",
    "        import re\n",
    "        \n",
    "        # Remove the extension first\n",
    "        name_without_ext = filename.replace('.png', '')\n",
    "        \n",
    "        # Check for _right or _left suffix\n",
    "        if name_without_ext.endswith('_right'):\n",
    "            base_name = name_without_ext[:-6]  # Remove '_right'\n",
    "            suffix_order = 0  # Right comes first\n",
    "        elif name_without_ext.endswith('_left'):\n",
    "            base_name = name_without_ext[:-5]  # Remove '_left'\n",
    "            suffix_order = 1  # Left comes second\n",
    "        else:\n",
    "            base_name = name_without_ext\n",
    "            suffix_order = 0  # No split, treat like right\n",
    "        \n",
    "        # ‚úÖ FIXED: Extract ALL numbers, get the last one (handles -bis, -ter, etc.)\n",
    "        numbers = re.findall(r'\\d+', base_name)\n",
    "        \n",
    "        if numbers:\n",
    "            page_num = int(numbers[-1])  # Get the LAST number found\n",
    "        else:\n",
    "            page_num = 0\n",
    "        \n",
    "        return (page_num, suffix_order)\n",
    "    \n",
    "    image_files = sorted(image_files, key=arabic_sort_key)\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"No images found in {split_dir}\")\n",
    "        return\n",
    "    \n",
    "    # Prepare OCR tasks\n",
    "    tasks = []\n",
    "    for img_path in image_files:\n",
    "        img_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        output_text_path = os.path.join(txt_dir, f\"{img_name}.txt\")\n",
    "        tasks.append((img_path, output_text_path))\n",
    "    \n",
    "    # Process images in parallel\n",
    "    with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
    "        futures = {executor.submit(process_image, in_path, out_path): (in_path, out_path) \n",
    "                  for in_path, out_path in tasks}\n",
    "        for future in as_completed(futures):\n",
    "            in_path, out_path = futures[future]\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"Unhandled error processing {in_path}: {e}\")\n",
    "    \n",
    "    # Combine all page texts\n",
    "    combined_text = []\n",
    "    for i, img_path in enumerate(image_files, 1):\n",
    "        img_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        text_path = os.path.join(txt_dir, f\"{img_name}.txt\")\n",
    "        if os.path.exists(text_path):\n",
    "            with open(text_path, 'r', encoding='utf-8') as f:\n",
    "                page_content = f.read()\n",
    "                combined_text.append(f\"=== Page {i} ({img_name}) ===\\n{page_content}\\n\")\n",
    "        else:\n",
    "            combined_text.append(f\"=== Page {i} ({img_name}) ===\\n[OCR failed for this page]\\n\")\n",
    "    \n",
    "    raw_text = \"\\n\".join(combined_text)\n",
    "    cleaned_text = clean_unnecessary_linebreaks(raw_text)\n",
    "    \n",
    "    # Save output\n",
    "    with open(output_md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(cleaned_text)\n",
    "    \n",
    "    print(f\"Saved OCR output to {output_md_path}\")\n",
    "\n",
    "def step1_pdf_to_split_images_all():\n",
    "    \"\"\"\n",
    "    Step 1: Convert all PDFs to images and apply splitting\n",
    "    Run this in one Jupyter cell\n",
    "    \"\"\"\n",
    "    os.makedirs(PDF_FOLDER, exist_ok=True)\n",
    "    os.makedirs(TEMP_FOLDER, exist_ok=True)\n",
    "    \n",
    "    pdf_files = sorted(glob.glob(os.path.join(PDF_FOLDER, \"*.pdf\")))\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(f\"No PDF files found in {PDF_FOLDER}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDFs to process.\")\n",
    "    \n",
    "    for pdf_path in tqdm(pdf_files, desc=\"Step 1: Converting & Splitting PDFs\"):\n",
    "        base_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "        temp_img_dir = os.path.join(TEMP_FOLDER, f\"tempo_{base_name}\")\n",
    "        split_dir = os.path.join(TEMP_FOLDER, f\"tempo_split_{base_name}\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing: {base_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            pdf_to_images_with_split(pdf_path, temp_img_dir, split_dir, base_name, zoom=4, debug=False)\n",
    "            print(f\"‚úÖ Step 1 complete for {base_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in step 1 for {base_name}: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 1 COMPLETE! All PDFs converted and split.\")\n",
    "    print(f\"Split images saved in: {TEMP_FOLDER}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "def step2_ocr_all_split_images():\n",
    "    \"\"\"\n",
    "    Step 2: Run OCR on all split images\n",
    "    Run this in another Jupyter cell\n",
    "    \"\"\"\n",
    "    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "    \n",
    "    # Find all split directories\n",
    "    split_dirs = sorted(glob.glob(os.path.join(TEMP_FOLDER, \"tempo_split_*\")))\n",
    "    \n",
    "    if not split_dirs:\n",
    "        print(f\"No split directories found in {TEMP_FOLDER}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(split_dirs)} PDFs to OCR process.\")\n",
    "    \n",
    "    for split_dir in tqdm(split_dirs, desc=\"Step 2: Running OCR\"):\n",
    "        # Extract PDF base name from directory name\n",
    "        dir_name = os.path.basename(split_dir)\n",
    "        pdf_base_name = dir_name.replace(\"tempo_split_\", \"\")\n",
    "        \n",
    "        output_md_path = os.path.join(OUTPUT_FOLDER, pdf_base_name + \".md\")\n",
    "        \n",
    "        if os.path.exists(output_md_path):\n",
    "            print(f\"Skipping {pdf_base_name} (already exists)\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"OCR Processing: {pdf_base_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            ocr_split_images(split_dir, pdf_base_name, output_md_path)\n",
    "            print(f\"‚úÖ Step 2 complete for {pdf_base_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in step 2 for {pdf_base_name}: {e}\")\n",
    "            with open(output_md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(f\"[OCR PROCESSING FAILED: {str(e)}]\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 2 COMPLETE! All OCR processing done.\")\n",
    "    print(f\"Output saved to: {OUTPUT_FOLDER}\")\n",
    "    print(\"=\"*60)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417cb785-a741-4e4b-ab58-69112bb5c67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "step1_pdf_to_split_images_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b920e2-2e14-40ee-a943-72f2ce1f720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "step2_ocr_all_split_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90783022-d6c9-45fb-b167-a9361519ddd3",
   "metadata": {},
   "source": [
    "### 4- Phase two : Runing OCR again on failed pages only using Deepseek OCR \n",
    "#### u can run deepseek on failed pages diffrently using deep seek VLLM serving instead of the transformer library \n",
    "#### create the deepseek_khalil VENV first check README"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab44af8-1cd5-4e43-88ad-3cb25ccc996d",
   "metadata": {},
   "source": [
    "#### Post processing code with deepseek \n",
    "##### DeepSeek OCR Cleanup Script\n",
    "\n",
    "**Purpose:** Processes failed OCR pages from Chandra using DeepSeek model as fallback.\n",
    "\n",
    "##### What it does:\n",
    "1. **Scans** all markdown files in each batch for `[OCR FAILED: Request timed out...]` entries\n",
    "2. **Extracts** failed page identifiers from `=== Page X (BO_XXX) ===` headers\n",
    "3. **Loads** DeepSeek model once per batch (efficient GPU usage)\n",
    "4. **Processes** failed images with 3-minute timeout per image\n",
    "5. **Converts** DeepSeek markdown output ‚Üí HTML to match Chandra's format\n",
    "6. **Updates** original markdown files by replacing failure messages with DeepSeek results\n",
    "7. **Cleans up** tempo folder after each batch completes\n",
    "8. **Rests** GPU for 3 minutes between batches\n",
    "\n",
    "##### Batch Processing Flow:\n",
    "```\n",
    "batch1 ‚Üí process ‚Üí delete tempo ‚Üí rest 3 min ‚Üí batch2 ‚Üí process ‚Üí delete tempo ‚Üí rest 3 min ‚Üí ...\n",
    "```\n",
    "\n",
    "##### Configuration:\n",
    "- Timeout: 180 seconds per image\n",
    "- Rest: 180 seconds between batches\n",
    "- Output: HTML-tagged text (matches Chandra format)\n",
    "- Handles: `-bis`/`-ter` filename variants, numbered folders (`processing_results_1`)\n",
    "\n",
    "##### Failure Handling:\n",
    "- If DeepSeek succeeds: Replaces `[OCR FAILED...]` with HTML output\n",
    "- If DeepSeek also fails: Marks as `[OCR FAILED WITH DEEPSEEK: Could not extract text]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4aeca5-f46e-4acc-899a-db46a8b5c12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "DeepSeek OCR Cleanup Script\n",
    "============================\n",
    "Processes failed OCR pages from Chandra using DeepSeek model.\n",
    "Processes one batch at a time in sorted order.\n",
    "Converts DeepSeek markdown output to HTML to match Chandra's output format.\n",
    "\"\"\"\n",
    "import shutil  \n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import torch\n",
    "import time\n",
    "import markdown\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "BATCHES_ROOT = \"/home/skiredj.abderrahman/khalil/OCR_scaling_bulletin_officiel/deepseek_post_ocr/batchs\"  # ‚ö†Ô∏è UPDATE THIS PATH\n",
    "DEEPSEEK_MODEL_NAME = \"/home/skiredj.abderrahman/khalil/DeepSeek-OCR\"\n",
    "CUDA_DEVICE = \"0\"\n",
    "DEEPSEEK_TIMEOUT_SECONDS = 180  # Timeout per image (3 minutes)\n",
    "BATCH_REST_SECONDS = 120  # Rest 2 minutes between batches\n",
    "\n",
    "# Set CUDA device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = CUDA_DEVICE\n",
    "\n",
    "# DeepSeek prompt\n",
    "DEEPSEEK_PROMPT = \"<image>\\n<|grounding|>Convert the document to markdown. \"\n",
    "\n",
    "\n",
    "def load_deepseek_model():\n",
    "    \"\"\"\n",
    "    Load DeepSeek OCR model once per batch\n",
    "    \"\"\"\n",
    "    print(\"Loading DeepSeek model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        DEEPSEEK_MODEL_NAME, \n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model = AutoModel.from_pretrained(\n",
    "        DEEPSEEK_MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        use_safetensors=True\n",
    "    )\n",
    "    model = model.eval().cuda().to(torch.bfloat16)\n",
    "    print(\"‚úÖ DeepSeek model loaded successfully!\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def markdown_to_html(markdown_text):\n",
    "    \"\"\"\n",
    "    Convert markdown text to HTML to match Chandra's output format\n",
    "    \"\"\"\n",
    "    try:\n",
    "        html_output = markdown.markdown(\n",
    "            markdown_text,\n",
    "            extensions=['tables', 'fenced_code', 'nl2br']\n",
    "        )\n",
    "        return html_output\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è  Markdown to HTML conversion failed: {e}\")\n",
    "        # Return original text wrapped in basic HTML if conversion fails\n",
    "        return f\"<div>{markdown_text}</div>\"\n",
    "\n",
    "\n",
    "def scan_markdown_for_failures(markdown_path):\n",
    "    \"\"\"\n",
    "    Scan a markdown file and extract all failed OCR pages.\n",
    "    Returns list of tuples: [(page_number, page_identifier), ...]\n",
    "    \"\"\"\n",
    "    with open(markdown_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    failures = []\n",
    "    for i, line in enumerate(lines):\n",
    "        # Look for OCR failure marker\n",
    "        if \"[OCR FAILED: Request timed out after 3 attempts]\" in line:\n",
    "            # Look back to find the page header\n",
    "            if i > 0:\n",
    "                prev_line = lines[i - 1]\n",
    "                # Extract page identifier from: === Page X (BO_XXXXX_Ar_YY_left) ===\n",
    "                match = re.search(r'=== Page \\d+ \\((.+?)\\) ===', prev_line)\n",
    "                if match:\n",
    "                    page_identifier = match.group(1)\n",
    "                    failures.append((i, page_identifier))\n",
    "    \n",
    "    return failures\n",
    "\n",
    "\n",
    "def get_pdf_base_name(page_identifier):\n",
    "    \"\"\"\n",
    "    Extract PDF base name from page identifier.\n",
    "    Examples:\n",
    "      - BO_5705_Ar_23_left ‚Üí BO_5705_Ar\n",
    "      - BO_5705_Ar_23 ‚Üí BO_5705_Ar\n",
    "      - BO_5705-bis_Ar_23_right ‚Üí BO_5705-bis_Ar\n",
    "    \"\"\"\n",
    "    # Remove _left or _right suffix if present\n",
    "    if page_identifier.endswith('_left'):\n",
    "        base = page_identifier[:-5]\n",
    "    elif page_identifier.endswith('_right'):\n",
    "        base = page_identifier[:-6]\n",
    "    else:\n",
    "        base = page_identifier\n",
    "    \n",
    "    # Remove the page number (last _XX)\n",
    "    parts = base.rsplit('_', 1)\n",
    "    if len(parts) == 2 and parts[1].isdigit():\n",
    "        return parts[0]\n",
    "    return base\n",
    "\n",
    "\n",
    "def run_deepseek_ocr(model, tokenizer, image_path, output_dir, timeout=180):\n",
    "    \"\"\"\n",
    "    Run DeepSeek OCR on a single image with timeout\n",
    "    \"\"\"\n",
    "    from concurrent.futures import ThreadPoolExecutor, TimeoutError\n",
    "    \n",
    "    def _run_inference():\n",
    "        return model.infer(\n",
    "            tokenizer,\n",
    "            prompt=DEEPSEEK_PROMPT,\n",
    "            image_file=image_path,\n",
    "            output_path=output_dir,\n",
    "            base_size=1024,\n",
    "            image_size=640,\n",
    "            crop_mode=True,\n",
    "            save_results=True,\n",
    "            test_compress=True\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        print(f\"  Running DeepSeek on: {os.path.basename(image_path)}\")\n",
    "        \n",
    "        # Run with timeout\n",
    "        with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "            future = executor.submit(_run_inference)\n",
    "            try:\n",
    "                res = future.result(timeout=timeout)\n",
    "                return res\n",
    "            except TimeoutError:\n",
    "                print(f\"  ‚è±Ô∏è  DeepSeek timed out after {timeout} seconds\")\n",
    "                return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå DeepSeek failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def update_markdown_file(markdown_path, deepseek_results):\n",
    "    \"\"\"\n",
    "    Update markdown file by replacing [OCR FAILED: ...] with DeepSeek results\n",
    "    deepseek_results: dict mapping page_identifier ‚Üí ocr_text\n",
    "    \"\"\"\n",
    "    with open(markdown_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    updated_lines = []\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i]\n",
    "        \n",
    "        # Check if this is a failure line\n",
    "        if \"[OCR FAILED: Request timed out after 3 attempts]\" in line:\n",
    "            # Extract page identifier from previous line\n",
    "            if i > 0:\n",
    "                prev_line = lines[i - 1]\n",
    "                match = re.search(r'=== Page \\d+ \\((.+?)\\) ===', prev_line)\n",
    "                if match:\n",
    "                    page_identifier = match.group(1)\n",
    "                    \n",
    "                    # Replace with DeepSeek result if available\n",
    "                    if page_identifier in deepseek_results:\n",
    "                        if deepseek_results[page_identifier] is not None:\n",
    "                            # Successful DeepSeek OCR\n",
    "                            updated_lines.append(deepseek_results[page_identifier] + '\\n')\n",
    "                        else:\n",
    "                            # DeepSeek also failed\n",
    "                            updated_lines.append(\"[OCR FAILED WITH DEEPSEEK: Could not extract text]\\n\")\n",
    "                    else:\n",
    "                        # Should not happen, but keep original if no result\n",
    "                        updated_lines.append(line)\n",
    "                else:\n",
    "                    updated_lines.append(line)\n",
    "            else:\n",
    "                updated_lines.append(line)\n",
    "        else:\n",
    "            updated_lines.append(line)\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    # Write updated content back\n",
    "    with open(markdown_path, 'w', encoding='utf-8') as f:\n",
    "        f.writelines(updated_lines)\n",
    "    \n",
    "    print(f\"  ‚úÖ Updated: {os.path.basename(markdown_path)}\")\n",
    "\n",
    "\n",
    "def process_batch(batch_path):\n",
    "    \"\"\"\n",
    "    Process a single batch folder\n",
    "    \"\"\"\n",
    "    batch_name = os.path.basename(batch_path)\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Processing Batch: {batch_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Paths - handle numbered folder suffixes\n",
    "    batch_num = re.search(r'batch(\\d+)', os.path.basename(batch_path)).group(1)\n",
    "    processing_results = os.path.join(batch_path, f\"processing_results_{batch_num}\")\n",
    "    markdown_dir = os.path.join(processing_results, \"documents_transformed_to_markdown\")\n",
    "    tempo_dir = os.path.join(processing_results, \"tempo\")\n",
    "    \n",
    "    if not os.path.exists(markdown_dir):\n",
    "        print(f\"‚ùå No markdown directory found in {batch_name}, skipping...\")\n",
    "        return\n",
    "    \n",
    "    # Step 1: Scan all markdowns for failures\n",
    "    print(\"\\nStep 1: Scanning markdowns for failed OCR pages...\")\n",
    "    all_failures = {}  # markdown_path ‚Üí [(line_num, page_identifier), ...]\n",
    "    \n",
    "    markdown_files = glob.glob(os.path.join(markdown_dir, \"*.md\"))\n",
    "    if not markdown_files:\n",
    "        print(f\"No markdown files found in {batch_name}\")\n",
    "        return\n",
    "    \n",
    "    for md_path in markdown_files:\n",
    "        failures = scan_markdown_for_failures(md_path)\n",
    "        if failures:\n",
    "            all_failures[md_path] = failures\n",
    "            print(f\"  Found {len(failures)} failures in {os.path.basename(md_path)}\")\n",
    "    \n",
    "    if not all_failures:\n",
    "        print(f\"‚úÖ No failures found in {batch_name}, skipping...\")\n",
    "        return\n",
    "    \n",
    "    total_failures = sum(len(f) for f in all_failures.values())\n",
    "    print(f\"\\nTotal failures to process: {total_failures}\")\n",
    "    \n",
    "    # Step 2: Load DeepSeek model\n",
    "    print(\"\\nStep 2: Loading DeepSeek model...\")\n",
    "    model, tokenizer = load_deepseek_model()\n",
    "    \n",
    "    # Step 3: Process each failed page\n",
    "    print(\"\\nStep 3: Running DeepSeek OCR on failed pages...\")\n",
    "    \n",
    "    for md_path, failures in all_failures.items():\n",
    "        md_basename = os.path.basename(md_path)\n",
    "        print(f\"\\nProcessing failures from: {md_basename}\")\n",
    "        \n",
    "        deepseek_results = {}  # page_identifier ‚Üí ocr_text\n",
    "        \n",
    "        for line_num, page_identifier in tqdm(failures, desc=f\"  {md_basename}\"):\n",
    "            # Get PDF base name\n",
    "            pdf_base = get_pdf_base_name(page_identifier)\n",
    "            \n",
    "            # Build image path\n",
    "            split_folder = os.path.join(tempo_dir, f\"tempo_split_{pdf_base}\")\n",
    "            image_path = os.path.join(split_folder, f\"{page_identifier}.png\")\n",
    "            \n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"  ‚ö†Ô∏è  Image not found: {image_path}\")\n",
    "                deepseek_results[page_identifier] = None\n",
    "                continue\n",
    "            \n",
    "            # Create output directory for DeepSeek\n",
    "            output_dir = os.path.join(tempo_dir, f\"tempo_res2_{pdf_base}\")\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            # Run DeepSeek OCR\n",
    "            result = run_deepseek_ocr(model, tokenizer, image_path, output_dir, timeout=DEEPSEEK_TIMEOUT_SECONDS)\n",
    "            \n",
    "            # Read result from result.mmd file (DeepSeek saves here)\n",
    "            result_mmd_path = os.path.join(output_dir, \"result.mmd\")\n",
    "            if os.path.exists(result_mmd_path):\n",
    "                try:\n",
    "                    with open(result_mmd_path, 'r', encoding='utf-8') as f:\n",
    "                        markdown_text = f.read()\n",
    "                    \n",
    "                    # Convert markdown to HTML to match Chandra's output format\n",
    "                    html_text = markdown_to_html(markdown_text)\n",
    "                    \n",
    "                    # Save both markdown and HTML for tracking\n",
    "                    output_text_path = os.path.join(output_dir, f\"{page_identifier}.txt\")\n",
    "                    with open(output_text_path, 'w', encoding='utf-8') as f:\n",
    "                        f.write(html_text)\n",
    "                    \n",
    "                    deepseek_results[page_identifier] = html_text\n",
    "                    print(f\"    ‚úÖ Success! Converted to HTML ({len(html_text)} characters)\")\n",
    "                except Exception as e:\n",
    "                    print(f\"    ‚ö†Ô∏è  Could not process result.mmd: {e}\")\n",
    "                    deepseek_results[page_identifier] = None\n",
    "            else:\n",
    "                print(f\"    ‚ùå result.mmd not found in {output_dir}\")\n",
    "                deepseek_results[page_identifier] = None\n",
    "        \n",
    "        # Step 4: Update markdown file\n",
    "        print(f\"\\nStep 4: Updating markdown file: {md_basename}\")\n",
    "        update_markdown_file(md_path, deepseek_results)\n",
    "    \n",
    "    # Cleanup\n",
    "    print(\"\\nCleaning up GPU memory...\")\n",
    "    del model\n",
    "    del tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"\\nDeleting tempo folder for {batch_name}...\")\n",
    "    if os.path.exists(tempo_dir):\n",
    "        shutil.rmtree(tempo_dir)\n",
    "        print(f\"‚úÖ Deleted: {tempo_dir}\")\n",
    "    print(f\"\\n‚úÖ Batch {batch_name} processing complete!\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to process batches one at a time in sorted order\n",
    "    \"\"\"\n",
    "    if not os.path.exists(BATCHES_ROOT):\n",
    "        print(f\"‚ùå Batches root directory not found: {BATCHES_ROOT}\")\n",
    "        return\n",
    "    \n",
    "    # Get all batch directories\n",
    "    batch_dirs = [d for d in glob.glob(os.path.join(BATCHES_ROOT, \"batch*\")) \n",
    "                  if os.path.isdir(d)]\n",
    "    \n",
    "    if not batch_dirs:\n",
    "        print(f\"‚ùå No batch directories found in {BATCHES_ROOT}\")\n",
    "        return\n",
    "    \n",
    "    # Sort batch directories\n",
    "    batch_dirs = sorted(batch_dirs, key=lambda x: int(re.search(r'batch(\\d+)', x).group(1)))\n",
    "    \n",
    "    print(f\"Found {len(batch_dirs)} batches to process:\")\n",
    "    for bd in batch_dirs:\n",
    "        print(f\"  - {os.path.basename(bd)}\")\n",
    "    \n",
    "    # Process each batch\n",
    "    for idx, batch_path in enumerate(batch_dirs):\n",
    "        try:\n",
    "            process_batch(batch_path)\n",
    "            \n",
    "            # Rest between batches (except after the last one)\n",
    "            if idx < len(batch_dirs) - 1:\n",
    "                print(f\"\\n{'='*70}\")\n",
    "                print(f\"üí§ Resting GPU for {BATCH_REST_SECONDS} seconds before next batch...\")\n",
    "                print(f\"{'='*70}\")\n",
    "                time.sleep(BATCH_REST_SECONDS)\n",
    "                \n",
    "        except Exception as e:\n",
    "            batch_name = os.path.basename(batch_path)\n",
    "            print(f\"\\n‚ùå Error processing {batch_name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            print(f\"Continuing to next batch...\\n\")\n",
    "            continue\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ALL BATCHES PROCESSED!\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    BATCHES_ROOT = \"/home/skiredj.abderrahman/khalil/OCR_scaling_bulletin_officiel/deepseek_post_ocr/dsKIKA/batchs\"  # ‚ö†Ô∏è UPDATE THIS PATH\n",
    "    DEEPSEEK_MODEL_NAME = \"/home/skiredj.abderrahman/khalil/DeepSeek-OCR\"\n",
    "    \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
